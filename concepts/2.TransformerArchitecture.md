
## Transformer Architecture

### 2.1 Why Transformers?

**Explanation**:  
Before transformers, RNNs and CNNs were commonly used for NLP. However, they had several limitations:

- **RNNs**: Process tokens sequentially, which is slow and struggles with long-term dependencies.
- **CNNs**: Have a limited context window.

Transformers solved these problems by:

- Using **self-attention** to process all tokens in parallel.
- Effectively capturing long-range dependencies.

**Example**:  
For the sentence **"The animal didn’t cross the street because it was too wide"**:

- An RNN might struggle to connect **"it"** with **"street"** due to sequential processing.
- A transformer uses self-attention to directly link these words regardless of their position.

---

### 2.2 Self-Attention Mechanism

**Explanation**:  
Self-attention computes **how much each token relates to every other token** in the sequence.

**Steps**:

1. **Create Query (Q), Key (K), Value (V) vectors** for each token.
2. Calculate **attention scores** by taking the dot product of Q and Kᵀ.
3. Apply **softmax** to these scores to get attention weights.
4. Multiply the attention weights with the corresponding V vectors to obtain the final output.

**Formula**:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

**Example**:  
For the input **"He threw the ball"**, the word **"threw"** will compute high attention scores with **"He"** and **"ball"**, effectively linking them based on context.

---

### 2.3 Encoder-Decoder Structure

**Explanation**:  
Transformers are typically organized in an encoder-decoder structure:

- **Encoder**:

  - Processes the input text.
  - Comprises layers with self-attention and feed-forward networks.
  - Used in models like BERT for understanding input context.

- **Decoder**:
  - Generates output text.
  - Contains self-attention layers (to process previous outputs) and cross-attention layers (to connect encoder outputs).
  - Used in models like GPT for text generation.

**Example**:  
In a machine translation task (English to French):

- The **encoder** processes the English sentence "Hello, how are you?"
- The **decoder** generates the French translation "Bonjour, comment ça va?" sequentially.

---

### MCQs for Transformers

1. **What is the purpose of self-attention?**  
   a) Generate text  
   b) Link tokens based on relevance  
   c) Reduce model size  
   d) Tokenize sentences

   **Answer**: **b) Link tokens based on relevance**  
   **Explanation**: Self-attention calculates relationships between tokens, connecting words (e.g., linking "it" to "cat") based on their contextual relevance.

2. **Which component generates the final output in a transformer model?**  
   a) Encoder  
   b) Decoder  
   c) Embedding layer  
   d) Loss function

   **Answer**: **b) Decoder**  
   **Explanation**: The decoder is responsible for generating output sequences, such as translated sentences or summaries.

3. **What problem did transformers primarily solve in NLP compared to RNNs?**  
   a) High computational cost  
   b) Sequential processing limitations  
   c) Lack of parallel computation  
   d) Insufficient dataset sizes

   **Answer**: **b) Sequential processing limitations**  
   **Explanation**: Transformers process tokens in parallel rather than sequentially, which overcomes the limitations of RNNs that process one token at a time.

---
