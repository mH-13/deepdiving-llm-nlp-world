## Core NLP Concepts

---

### 1.1 What is NLP?

**Explanation**:  
NLP (Natural Language Processing) is a subfield of AI that focuses on enabling computers to **understand, interpret, and generate human language**. It bridges the gap between human communication and machine understanding.

**Applications**:

- **Spam Detection**: Classifying emails as spam or not.
- **Machine Translation**: Translating text between languages (e.g., Google Translate).
- **Chatbots**: Systems like ChatGPT that converse with users.

**Example**:  
Imagine a system that reads restaurant reviews and labels them as "Positive" or "Negative." This is **sentiment analysis**, a classic NLP task.

---

### 1.2 NLU vs. NLG

**Explanation**:

- **NLU (Natural Language Understanding)**:
  - Focuses on **comprehending** text.
  - Tasks: Sentiment analysis, Named Entity Recognition (NER), question answering.
- **NLG (Natural Language Generation)**:
  - Focuses on **creating** text.
  - Tasks: Text summarization, story generation, chatbots.

**Example**:

- **NLU**: Extracting "Paris" as a location from "I visited Paris last summer."
- **NLG**: Generating a weather report like "Tomorrow will be sunny with a high of 25°C."

---

### 1.3 Tokenization

**Explanation**:  
Tokenization splits raw text into smaller units called **tokens** (words, subwords, or characters). This is the first step in NLP pipelines.

**Types**:

- **Word Tokenization**: Splitting by spaces/punctuation (e.g., "Hello, world!" → `["Hello", ",", "world", "!"]`).
- **Subword Tokenization**: Breaking rare words into smaller parts (e.g., "unhappiness" → `["un", "happiness"]`).

**Example**:  
For the sentence **"Don't panic!"**:

- **Word Tokenization**: `["Don't", "panic", "!"]`
- **Subword Tokenization (BERT-style)**: `["Don", "'", "t", "panic", "!"]`

---

### 1.4 Embeddings

**Explanation**:  
Embeddings convert words into **numerical vectors** to capture their meanings. Similar words have similar vectors.

**Why?**  
Machines can’t process raw text. Embeddings turn words into math-friendly formats.

**Example**:

- The word "king" might be represented as `[0.25, -0.1, 0.7]`.
- "Queen" could be `[0.24, -0.09, 0.69]` (close to "king" in vector space).

**Common Embedding Models**:

- **Word2Vec**: Maps words based on context.
- **BERT**: Generates context-aware embeddings (e.g., "bank" in "river bank" vs. "bank account" has different vectors).

---

### 1.5 Attention Mechanism

**Explanation**:  
Attention allows models to **focus on relevant parts of the input** when processing data. It assigns weights to tokens to highlight their importance.

**Example**:  
For the sentence **"The cat sat on the mat because it was tired"**:

- The word **"it"** refers to **"cat"**.
- Attention weights will link **"it"** → **"cat"** strongly.

**Visualization**:

| Token | Attention Weights    |
| ----- | -------------------- |
| The   | [0.1, 0.0, 0.0, ...] |
| cat   | [0.3, 0.4, 0.1, ...] |
| sat   | [0.2, 0.3, 0.0, ...] |
| ...   | ...                  |

---

### MCQs for Core NLP Concepts

1. **Which task is an example of NLU?**  
   a) Text summarization  
   b) Sentiment analysis  
   c) Poetry generation  
   d) Machine translation

   **Answer**: **b) Sentiment analysis**  
   **Explanation**: NLU involves understanding text (e.g., classifying sentiment), whereas NLG involves generating text (e.g., poetry).

2. **What does tokenization do?**  
   a) Translate text to another language  
   b) Split text into tokens  
   c) Generate embeddings  
   d) Remove stopwords

   **Answer**: **b) Split text into tokens**  
   **Explanation**: Tokenization breaks raw text into smaller units like words or subwords.

3. **Why are embeddings used in NLP?**  
   a) To reduce model size  
   b) To convert text into numerical vectors  
   c) To generate text  
   d) To tokenize sentences

   **Answer**: **b) To convert text into numerical vectors**  
   **Explanation**: Embeddings represent words as vectors so machines can process them mathematically.

---