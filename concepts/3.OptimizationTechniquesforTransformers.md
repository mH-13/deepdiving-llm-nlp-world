# Optimization Techniques, LLM Fine-tuning & Benchmarking


##  Optimization Techniques for Transformers

### 3.1 Model Pruning

**Explanation**:  
Pruning removes "unimportant" parts of a neural network (e.g., neurons or attention heads) to reduce model size and computational cost while retaining performance.

**How It Works**:  
1. Train the model fully.  
2. Identify components (e.g., weights, layers) with minimal impact on outputs.  
3. Remove these components and perform light retraining to recover accuracy.

**Example**:  
- Pruning 30% of BERT’s attention heads can reduce model size by approximately 20% with minimal accuracy drop.

---

### 3.2 Quantization

**Explanation**:  
Quantization reduces the numerical precision of weights (e.g., from 32-bit to 8-bit) which results in a smaller model size and faster inference.

**Types**:  
- **Post-training quantization**: Applied after the model is fully trained (e.g., via TensorFlow Lite).  
- **Quantization-aware training**: Simulates lower precision during training, improving final model performance at reduced precision.

**Example**:  
- A 1GB FP32 model can become as small as 250MB in INT8 format after quantization.

---

### 3.3 Knowledge Distillation

**Explanation**:  
In knowledge distillation, a smaller "student" model is trained to mimic the outputs of a larger "teacher" model. This process helps retain most of the teacher's performance with a significantly reduced number of parameters.

**Steps**:  
1. Train the teacher model (e.g., BERT).  
2. Use the teacher’s outputs (or logits) as soft targets while training the student model (e.g., DistilBERT).

**Example**:  
- DistilBERT achieves about 95% of BERT's performance while using 40% fewer parameters.

---

### 3.4 Mixed Precision Training

**Explanation**:  
Mixed precision training uses both 16-bit (FP16) and 32-bit (FP32) floating-point numbers during training. FP16 is used for faster computations while FP32 ensures numerical stability, especially for gradients.

**Tools**:  
- NVIDIA’s Apex library or PyTorch’s built-in `autocast` functionality.

**Example**:  
- Training time for models like GPT-2 can be reduced by up to 2x using mixed precision training.

---

### MCQs for Optimization Techniques

1. **What is the primary goal of model pruning?**  
   a) Increase model size  
   b) Reduce computational cost  
   c) Improve accuracy  
   d) Generate text  

   **Answer**: **b) Reduce computational cost**  
   **Explanation**: Pruning removes redundant parts of the model to make it smaller and faster without significantly compromising performance.

2. **Which technique converts 32-bit weights to 8-bit?**  
   a) Knowledge Distillation  
   b) Quantization  
   c) Pruning  
   d) Mixed Precision Training  

   **Answer**: **b) Quantization**  
   **Explanation**: Quantization reduces the numerical precision of weights (e.g., from 32-bit to 8-bit), leading to smaller models and faster inference.

3. **What does the "student" model do in knowledge distillation?**  
   a) Teaches the teacher model  
   b) Mimics the teacher’s outputs  
   c) Generates training data  
   d) Tokenizes input text  

   **Answer**: **b) Mimics the teacher’s outputs**  
   **Explanation**: The student model learns from the teacher model’s predictions (soft targets), reducing the overall number of parameters while maintaining performance.

---
